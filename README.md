# ðŸ§  Data Engineering Projects Collection  
## ðŸš€ End-to-End Data Pipeline with Airflow, Docker, and GCP

Build an automated data pipeline using **Apache Airflow**, **Docker**, and **Google Cloud Platform (GCP)**  
to extract public API data, transform it with Python, and load it into **Cloud SQL** for analytics and dashboarding.

---

### ðŸ”§ Stack Used
- Apache Airflow 2.8+
- Docker + Docker Compose
- Python 3.8+
- Google Cloud (Compute Engine, Cloud SQL, Cloud Storage)
- PostgreSQL or MSSQL
- Looker Studio (Dashboard)

---

### ðŸ“Œ Project Goals
- [x] Automate data ingestion from public APIs
- [x] Run DAGs using Airflow on Docker in GCP VM
- [x] Store data in Cloud SQL
- [x] Optional: Visualize with Looker Studio / Google Sheets

---

### ðŸ“‚ Folder Structure
```plaintext
.
â”œâ”€â”€ dags/                # All Airflow DAGs
â”œâ”€â”€ docker-compose.yaml  # Services: Airflow, Postgres
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ setup/               # Shell scripts / config for GCP
â”œâ”€â”€ README.md
